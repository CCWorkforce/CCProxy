OPENAI_API_KEY=<your openrouter api key>
BIG_MODEL_NAME=gpt-5-2025-08-07
SMALL_MODEL_NAME=gpt-5-mini-2025-08-07
LOG_LEVEL=DEBUG # could be DEBUG, INFO, ERROR, WARNING
LOG_PRETTY_CONSOLE=True
ERROR_LOG_FILE_PATH=error.jsonl
OPENAI_BASE_URL=https://api.openai.com/v1

IS_LOCAL_DEPLOYMENT=True # Set to True if it's a local deployment, otherwise set it to False

# Guardrail / security defaults
RATE_LIMIT_ENABLED=true
RATE_LIMIT_PER_MINUTE=60
RATE_LIMIT_BURST=30
SECURITY_HEADERS_ENABLED=true
ENABLE_HSTS=false
ENABLE_CORS=false
CORS_ALLOW_ORIGINS=http://localhost:11434,http://127.0.0.1:11434
CORS_ALLOW_METHODS=POST,OPTIONS
CORS_ALLOW_HEADERS=Authorization,Content-Type,X-Requested-With
ALLOWED_HOSTS=localhost,127.0.0.1
RESTRICT_BASE_URL=true
ALLOWED_BASE_URL_HOSTS=api.openai.com,openrouter.ai,api.deepseek.com
REDACT_LOG_FIELDS=openai_api_key,authorization
MAX_STREAM_SECONDS=600

# Caching and performance
CACHE_TOKEN_COUNTS_ENABLED=true
CACHE_TOKEN_COUNTS_TTL_S=300
CACHE_TOKEN_COUNTS_MAX=2048
CACHE_CONVERTERS_ENABLED=true
CACHE_CONVERTERS_MAX=256
STREAM_DEDUPE_ENABLED=true
METRICS_CACHE_ENABLED=true

# Token truncation
TRUNCATE_LONG_REQUESTS=true
TRUNCATION_CONFIG={"strategy": "oldest_first", "min_tokens": 100, "system_message_priority": true}

# Provider retry policy
PROVIDER_MAX_RETRIES=3
PROVIDER_RETRY_BASE_DELAY=1.0
PROVIDER_RETRY_JITTER=0.5

# Circuit breaker configuration
CIRCUIT_BREAKER_FAILURE_THRESHOLD=5  # Number of failures before opening circuit
CIRCUIT_BREAKER_RECOVERY_TIMEOUT=60  # Seconds to wait before attempting recovery
CIRCUIT_BREAKER_HALF_OPEN_REQUESTS=3  # Requests to test during recovery

# Connection pool configuration
POOL_MAX_KEEPALIVE_CONNECTIONS=50  # Max persistent connections (higher for production)
POOL_MAX_CONNECTIONS=500  # Total max connections (adjust based on load)
POOL_KEEPALIVE_EXPIRY=120  # Seconds to keep connections alive

# HTTP timeout configuration
HTTP_CONNECT_TIMEOUT=10.0  # Seconds to wait for connection
HTTP_WRITE_TIMEOUT=30.0  # Seconds to wait for write operations
HTTP_POOL_TIMEOUT=10.0  # Seconds to wait for connection from pool

# Distributed tracing configuration (OpenTelemetry)
TRACING_ENABLED=false  # Enable distributed tracing
TRACING_EXPORTER=console  # Options: console, otlp, jaeger
TRACING_ENDPOINT=  # Endpoint for OTLP/Jaeger (e.g., localhost:6831)
TRACING_SERVICE_NAME=ccproxy  # Service name for traces

# Client-side rate limiting (optimized for agentic coding tools with large contexts)
CLIENT_RATE_LIMIT_ENABLED=true  # Enable client-side rate limiting
CLIENT_RATE_LIMIT_RPM=120  # Requests per minute (2 req/sec for coding assistants)
CLIENT_RATE_LIMIT_TPM=6000000  # Tokens per minute (supports ~50K tokens/req average, bursts up to 200K)
CLIENT_RATE_LIMIT_BURST=30  # Burst capacity for rapid successive requests
CLIENT_RATE_LIMIT_ADAPTIVE=true  # Adapt limits based on 429 responses
