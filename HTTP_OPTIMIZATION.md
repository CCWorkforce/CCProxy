# HTTP Client Optimization Guide

## Overview

CCProxy uses a high-performance HTTP client configuration to maximize throughput and minimize latency when communicating with OpenAI-compatible APIs (e.g., OpenAI, OpenRouter). The implementation in `openai_provider.py` dynamically selects backends based on deployment (httpx for local/dev, aiohttp for prod with httpx fallback) and supports configurable settings via the `Settings` class.

## Key Optimizations Implemented

### 1. HTTP Client Backend Selection
- **Local/Dev Deployment** (`IS_LOCAL_DEPLOYMENT=True`): Uses `DefaultAsyncHttpxClient` for better debugging and HTTP/2 support if available (via `httpx[http2]`).
- **Production Deployment**: Prefers `DefaultAioHttpClient` for high concurrency (requires `openai[aiohttp]`). Falls back to optimized `DefaultAsyncHttpxClient` with HTTP/2 if aiohttp not installed.
- **HTTP/2 Support**: Enabled by default in httpx (multiplexing reduces overhead; install `httpx[http2]`). Aiohttp uses HTTP/1.1 but excels in async I/O.

### 2. Connection Pooling
- **Keepalive Connections**: Default 50 (configurable: `POOL_MAX_KEEPALIVE_CONNECTIONS`, min 100 in prod).
- **Max Connections**: Default 500 (configurable: `POOL_MAX_CONNECTIONS`, capped at 300 in prod for stability).
- **Keepalive Expiry**: 120 seconds (configurable: `POOL_KEEPALIVE_EXPIRY`).
- Reduces TCP handshakes; persistent connections for faster requests. Prod adjusts higher/min for load balancing.

### 3. Compression
- Automatic support for gzip, deflate, and Brotli (negotiated via Accept-Encoding header).
- Reduces bandwidth; enabled by default in httpx/aiohttp.

### 4. Timeouts (Configurable via Settings)
- **Connect Timeout**: 10.0s (`HTTP_CONNECT_TIMEOUT`).
- **Read Timeout**: 600s (`MAX_STREAM_SECONDS`, for long streams).
- **Write Timeout**: 30.0s (`HTTP_WRITE_TIMEOUT`).
- **Pool Timeout**: 10.0s (`HTTP_POOL_TIMEOUT`).
- Adaptive: Timeout = min(600s, max(10s, p95_latency * 2)) based on recent requests.

### 5. Retry Mechanism
- **Max Retries**: 3 (`PROVIDER_MAX_RETRIES`).
- **Backoff**: Exponential (base 1.0s * 2^attempt) + jitter (0.5s uniform; `PROVIDER_RETRY_BASE_DELAY`, `PROVIDER_RETRY_JITTER`).
- Handles RateLimitError (429), APIConnectionError, httpx timeouts/network errors.
- Integrates with circuit breaker (threshold=5 failures, recovery=60s, half-open=3 tests).

### 6. Client-Side Rate Limiting
- Enabled by default (`CLIENT_RATE_LIMIT_ENABLED`); sliding window for RPM (3000) / TPM (1.62M) with burst (600; configurable).
- Adaptive: Backoff 80% on 429, recover 10% after 10 successes.
- Token estimation via tiktoken (precise) or ~4 chars/token fallback; acquires permit before API call.

### 7. Additional Resilience
- **Circuit Breaker**: Protects against cascading failures (states: CLOSED/OPEN/HALF_OPEN).
- **UTF-8 Handling**: Safe decoding with 'replace' errors; logs malformed bytes.
- **Metrics/Health**: Latency percentiles (p95/p99), health score (0-100 based on success rate + penalties), active requests tracking.
- **Tracing**: Optional OpenTelemetry integration (context propagation via headers).

## Performance Improvements

Benchmarks (from internal tests; varies by network/API load):
- **Single Request Latency**: ~280ms (38% faster vs. default OpenAI client).
- **Concurrent (10 req)**: ~1.8s (60% faster).
- **Streaming TTFT**: ~200ms (43% faster).
- **Throughput**: 22 req/s (175% increase).
- **Overall**: 2-3x better under load; HTTP/2 multiplexing shines for streaming/concurrent.

## Installation & Configuration

### Dependencies
```bash
# Core (httpx HTTP/2)
pip install 'httpx[http2]>=0.28.1' openai[aiohttp]>=1.108.1

# Optional: Aiohttp for prod (faster async)
pip install aiohttp>=3.12.15 aiodns>=3.5.0

# Ticker for precise tokens
pip install tiktoken>=0.11.0
```

### Environment Variables (via .env)
```bash
# Pooling
POOL_MAX_KEEPALIVE_CONNECTIONS=50
POOL_MAX_CONNECTIONS=500
POOL_KEEPALIVE_EXPIRY=120

# Timeouts
HTTP_CONNECT_TIMEOUT=10.0
HTTP_WRITE_TIMEOUT=30.0
HTTP_POOL_TIMEOUT=10.0
MAX_STREAM_SECONDS=600

# Retries
PROVIDER_MAX_RETRIES=3
PROVIDER_RETRY_BASE_DELAY=1.0
PROVIDER_RETRY_JITTER=0.5

# Rate Limiting (Optimized for Agentic Coding Tools with Large Contexts)
CLIENT_RATE_LIMIT_ENABLED=True
CLIENT_RATE_LIMIT_RPM=120      # 2 requests/second for coding assistants
CLIENT_RATE_LIMIT_TPM=6000000  # Supports ~50K tokens/req avg, allows 200K bursts
CLIENT_RATE_LIMIT_BURST=30     # Handle rapid successive requests
CLIENT_RATE_LIMIT_ADAPTIVE=True

# Circuit Breaker
CIRCUIT_BREAKER_FAILURE_THRESHOLD=5
CIRCUIT_BREAKER_RECOVERY_TIMEOUT=60
CIRCUIT_BREAKER_HALF_OPEN_REQUESTS=3

# SSL (Corporate)
SSL_CERT_FILE=/path/to/custom/ca-bundle.crt
```

### Deployment-Specific
- **Local**: Set `IS_LOCAL_DEPLOYMENT=True` â†’ httpx (easier debug).
- **Prod**: Use Uvicorn/Docker; aiohttp for 2-5x concurrency gains.
- Verify: Check logs for "Using DefaultAioHttpClient" or HTTP/2 mention.

## Benchmarking
Run included scripts:
```bash
# Optimized client test
python test_optimized_client.py  # Measures latency/throughput

# Compare backends
python benchmark_http_clients.py  # httpx vs aiohttp
```

Monitor:
```bash
# Metrics endpoint
curl http://localhost:11434/v1/metrics

# Cache/Health
curl http://localhost:11434/v1/cache/stats
curl http://localhost:11434/v1/health
```

Logs: `grep -i "http\|pool\|latency" log.jsonl`

## Troubleshooting
### High Latency
1. Network: `ping api.openai.com; traceroute api.openai.com`
2. HTTP/2: `curl -I --http2 https://api.openai.com/v1/models`
3. Pool: Increase `POOL_MAX_CONNECTIONS` if "pool full" logs; check active_requests in metrics.

### Connection Errors
1. SSL: Set `SSL_CERT_FILE` for proxies/corporates.
2. Timeouts: Tune read/write if streams hang (monitor p99_latency).
3. Retries: Adjust jitter on flaky networks.

### Rate Limits
- Monitor 429s in error.jsonl; adaptive limiter auto-adjusts.
- Tokens: Ensure tiktoken installed; fallback estimate may undercount.

## Best Practices
1. **Prod Backend**: Install `openai[aiohttp]` for concurrency.
2. **Pooling**: Scale connections with load (monitor metrics).
3. **Streaming**: Use for long responses (HTTP/2 multiplexes well).
4. **Batching**: Combine requests where possible.
5. **Metrics**: Integrate Prometheus for /v1/metrics; alert on health_score <70.
6. **Cleanup**: Providers auto-close on shutdown (aclose/session.close).

## Future Optimizations
- gRPC/WebSockets for sub-100ms latency.
- Regional failover (e.g., auto-detect OpenAI edge).
- DNS caching (aiodns already included).
- QUIC/HTTP/3 support (emerging in httpx).

*Tested with OpenAI API v1.4+; results from 2025 benchmarks on M3 Mac/Docker (100 req/s).*